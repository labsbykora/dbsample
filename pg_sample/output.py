"""SQL output generation for schema and data."""

import json
import re
import fnmatch
from typing import List, Dict, Optional, TextIO
from datetime import datetime
import psycopg
from pg_sample.schema import Table
from pg_sample.sampling import SamplingResult
from pg_sample.dependencies import DependencyResolver


class SQLOutputGenerator:
    """Generate SQL output for schema and sampled data."""
    
    def __init__(
        self,
        conn: psycopg.Connection,
        tables: List[Table],
        results: Dict[str, SamplingResult],
        resolver: DependencyResolver,
        schema_objects: Dict[str, List[Dict[str, any]]],
        data_only: bool = False,
        target_version: Optional[str] = None,
        logger: Optional[any] = None,
        verbose: bool = False,
        export_metadata: Optional[Dict[str, any]] = None,
        exclude_columns: Optional[List[str]] = None,
    ):
        """Initialize SQL output generator.
        
        Args:
            conn: Database connection
            tables: List of tables
            results: Sampling results
            resolver: Dependency resolver
            schema_objects: Additional database objects
            data_only: Export data only (no schema)
            target_version: Target PostgreSQL version
            logger: Logger instance for verbose output
            verbose: Enable verbose logging
            export_metadata: Optional metadata about the export (dbname, host, etc.)
            exclude_columns: List of column exclusion patterns
        """
        self.conn = conn
        self.tables = {t.qualified_name: t for t in tables}
        self.results = results
        self.resolver = resolver
        self.schema_objects = schema_objects
        self.data_only = data_only
        self.target_version = target_version
        self.logger = logger
        self.verbose = verbose
        self.export_metadata = export_metadata or {}
        self.exclude_columns = exclude_columns or []
    
    def generate(self, output: TextIO, encoding: str = "utf-8"):
        """Generate SQL output.
        
        Args:
            output: Output file-like object
            encoding: Character encoding
        """
        if self.verbose and self.logger:
            self.logger.info("  Writing SQL header...")
        # Write header
        self._write_header(output)
        
        if not self.data_only:
            if self.verbose and self.logger:
                self.logger.info("  Writing schema definitions...")
            # Write schema
            if self.verbose and self.logger:
                self.logger.info("    Extensions...")
            self._write_extensions(output)
            if self.verbose and self.logger:
                self.logger.info("    Types...")
            self._write_types(output)
            if self.verbose and self.logger:
                self.logger.info("    Tables...")
            self._write_tables(output)
            if self.verbose and self.logger:
                self.logger.info("    Sequences...")
            self._write_sequences(output)
            if self.verbose and self.logger:
                self.logger.info("    Views...")
            self._write_views(output)
            if self.verbose and self.logger:
                self.logger.info("    Materialized views...")
            self._write_materialized_views(output)
        
        # Write data
        if self.verbose and self.logger:
            self.logger.info("  Writing data (INSERT statements)...")
        self._write_data(output)
        
        # Write constraints (if not data_only)
        if not self.data_only:
            if self.verbose and self.logger:
                self.logger.info("  Writing constraints...")
            self._write_constraints(output)
        
        # Write footer
        if self.verbose and self.logger:
            self.logger.info("  Writing footer...")
        self._write_footer(output)
    
    def _write_header(self, output: TextIO):
        """Write SQL header comments with export metadata."""
        # Get version from package
        try:
            from pg_sample import __version__
            version = __version__
        except ImportError:
            version = "1.0.0"
        
        output.write("-- ============================================================================\n")
        output.write("-- PostgreSQL Database Sample Export\n")
        output.write(f"-- Generated by pg-sample v{version}\n")
        output.write(f"-- Generated: {datetime.utcnow().isoformat()}Z\n")
        output.write("--\n")
        
        # Database information
        dbname = self.export_metadata.get("dbname")
        host = self.export_metadata.get("host")
        if dbname:
            output.write(f"-- Source Database: {dbname}")
            if host:
                output.write(f" @ {host}")
            output.write("\n")
        
        # Export mode
        output.write(f"-- Export Mode: {'Data Only' if self.data_only else 'Schema + Data'}\n")
        
        # Target version
        if self.target_version:
            output.write(f"-- Target PostgreSQL Version: {self.target_version}\n")
        
        # Statistics
        total_tables = len(self.tables)
        tables_with_data = len([r for r in self.results.values() if r.rows])
        total_rows = sum(r.row_count for r in self.results.values())
        
        output.write("--\n")
        output.write("-- Export Statistics:\n")
        output.write(f"--   Tables: {total_tables} total, {tables_with_data} with data\n")
        output.write(f"--   Total Rows: {total_rows:,}\n")
        
        # Database objects
        if not self.data_only:
            obj_counts = {
                "Types": len(self.schema_objects.get("types", [])),
                "Sequences": len(self.schema_objects.get("sequences", [])),
                "Views": len(self.schema_objects.get("views", [])),
                "Materialized Views": len(self.schema_objects.get("materialized_views", [])),
                "Functions": len(self.schema_objects.get("functions", [])),
                "Extensions": len(self.schema_objects.get("extensions", [])),
            }
            obj_list = [f"{name}: {count}" for name, count in obj_counts.items() if count > 0]
            if obj_list:
                output.write(f"--   Objects: {', '.join(obj_list)}\n")
        
        # Sampling options
        sampling_info = []
        if self.export_metadata.get("limit_rules"):
            limit_str = ", ".join(self.export_metadata["limit_rules"])
            sampling_info.append(f"Limits: {limit_str}")
        if self.export_metadata.get("ordered"):
            order = "DESC" if self.export_metadata.get("ordered_desc", True) else "ASC"
            sampling_info.append(f"Ordered: {order}")
        if self.export_metadata.get("random"):
            sampling_info.append("Random sampling")
        
        if sampling_info:
            output.write(f"--   Sampling: {', '.join(sampling_info)}\n")
        
        # Exclusions
        exclusions = []
        if self.export_metadata.get("exclude_schemas"):
            exclusions.append(f"Schemas: {', '.join(self.export_metadata['exclude_schemas'])}")
        if self.export_metadata.get("exclude_tables"):
            exclusions.append(f"Tables: {', '.join(self.export_metadata['exclude_tables'])}")
        if self.export_metadata.get("exclude_columns"):
            exclusions.append(f"Columns: {', '.join(self.export_metadata['exclude_columns'])}")
        
        if exclusions:
            output.write(f"--   Excluded: {', '.join(exclusions)}\n")
        
        output.write("-- ============================================================================\n")
        output.write("\n")
        output.write("BEGIN;\n\n")
        output.write("-- Disable triggers during data load\n")
        output.write("SET session_replication_role = 'replica';\n\n")
    
    def _write_footer(self, output: TextIO):
        """Write SQL footer."""
        output.write("\n-- Re-enable triggers\n")
        output.write("SET session_replication_role = 'origin';\n\n")
        output.write("COMMIT;\n")
    
    def _write_extensions(self, output: TextIO):
        """Write extension CREATE statements."""
        if not self.schema_objects.get("extensions"):
            return
        
        output.write("-- Extensions\n")
        for ext in self.schema_objects["extensions"]:
            output.write(f'CREATE EXTENSION IF NOT EXISTS "{ext["name"]}" WITH VERSION \'{ext["version"]}\';\n')
        output.write("\n")
    
    def _write_types(self, output: TextIO):
        """Write custom type definitions."""
        if not self.schema_objects.get("types"):
            return
        
        output.write("-- Custom Types\n")
        # Use the type definition we already discovered
        # For composite types and domains, we'll output a simplified version
        with self.conn.cursor() as cur:
            for type_obj in self.schema_objects["types"]:
                schema = type_obj["schema"]
                name = type_obj["name"]
                definition = type_obj.get("definition", "")
                
                # Try to get the full type definition
                try:
                    cur.execute("""
                        SELECT 
                            t.typtype,
                            t.typrelid,
                            CASE 
                                WHEN t.typtype = 'c' THEN 
                                    (SELECT string_agg(attname || ' ' || pg_catalog.format_type(atttypid, atttypmod), ', ' ORDER BY attnum)
                                     FROM pg_attribute 
                                     WHERE attrelid = t.typrelid AND attnum > 0 AND NOT attisdropped)
                                WHEN t.typtype = 'd' THEN
                                    pg_catalog.format_type(t.typbasetype, t.typtypmod) || 
                                    CASE WHEN t.typnotnull THEN ' NOT NULL' ELSE '' END ||
                                    CASE WHEN t.typdefault IS NOT NULL THEN ' DEFAULT ' || t.typdefault::text ELSE '' END
                                WHEN t.typtype = 'e' THEN
                                    (SELECT string_agg('''' || enumlabel || '''', ', ' ORDER BY enumsortorder)
                                     FROM pg_enum
                                     WHERE enumtypid = t.oid)
                                ELSE NULL
                            END as type_def,
                            CASE 
                                WHEN t.typtype = 'c' AND t.typrelid != 0 THEN
                                    (SELECT relkind FROM pg_class WHERE oid = t.typrelid)
                                ELSE NULL
                            END as relkind
                        FROM pg_type t
                        JOIN pg_namespace n ON n.oid = t.typnamespace
                        WHERE n.nspname = %s AND t.typname = %s
                    """, [schema, name])
                    row = cur.fetchone()
                    
                    if not row:
                        # Type not found, skip it
                        continue
                    
                    typtype = row[0]
                    type_def = row[2]
                    relkind = row[3]
                    
                    # Skip table types - composite types that are actually table types
                    # should not be exported as CREATE TYPE
                    if typtype == 'c' and relkind == 'r':
                        # This is a table type, skip it
                        continue
                    
                    # Only write if we have a valid type definition
                    if type_def:
                        if typtype == 'c':  # composite type
                            output.write(f'CREATE TYPE "{schema}"."{name}" AS ({type_def});\n')
                        elif typtype == 'd':  # domain
                            output.write(f'CREATE DOMAIN "{schema}"."{name}" AS {type_def};\n')
                        elif typtype == 'e':  # enum type
                            output.write(f'CREATE TYPE "{schema}"."{name}" AS ENUM ({type_def});\n')
                        # If typtype is something else and we have a definition, skip it
                        # as we don't know how to handle it properly
                    # If no valid type_def, skip this type rather than generating invalid SQL
                except Exception as e:
                    # If we can't get detailed definition, skip this type
                    # rather than generating invalid SQL
                    # Log the error would be nice but we don't have logger here
                    pass
        output.write("\n")
    
    def _write_tables(self, output: TextIO):
        """Write table CREATE statements."""
        output.write("-- Tables\n")
        
        insertion_order = self.resolver.get_insertion_order()
        
        for table_name in insertion_order:
            if table_name not in self.tables:
                continue
            
            table = self.tables[table_name]
            self._write_table_definition(output, table)
        
        output.write("\n")
    
    def _write_table_definition(self, output: TextIO, table: Table):
        """Write CREATE TABLE statement."""
        output.write(f'CREATE TABLE "{table.schema}"."{table.name}" (\n')
        
        column_defs = []
        for col in table.columns:
            col_def = f'  "{col["name"]}" {col["type"]}'
            if col["not_null"]:
                col_def += " NOT NULL"
            if col["default"]:
                col_def += f" DEFAULT {col['default']}"
            column_defs.append(col_def)
        
        output.write(",\n".join(column_defs))
        output.write("\n);\n\n")
        
        # Write check constraints (in table definition for some versions)
        for check in table.check_constraints:
            output.write(f'ALTER TABLE "{table.schema}"."{table.name}" ADD CONSTRAINT "{check["name"]}" {check["definition"]};\n')
    
    def _write_sequences(self, output: TextIO):
        """Write sequence definitions and setval calls."""
        if not self.schema_objects.get("sequences"):
            return
        
        output.write("-- Sequences\n")
        # Sequences are typically auto-created with SERIAL columns
        # We'll handle setval calls after data load
        output.write("\n")
    
    def _write_views(self, output: TextIO):
        """Write view definitions."""
        if not self.schema_objects.get("views"):
            return
        
        output.write("-- Views\n")
        for view in self.schema_objects["views"]:
            output.write(f'CREATE VIEW "{view["schema"]}"."{view["name"]}" AS {view["definition"]};\n')
        output.write("\n")
    
    def _write_materialized_views(self, output: TextIO):
        """Write materialized view definitions."""
        if not self.schema_objects.get("materialized_views"):
            return
        
        output.write("-- Materialized Views\n")
        for mv in self.schema_objects["materialized_views"]:
            output.write(f'CREATE MATERIALIZED VIEW "{mv["schema"]}"."{mv["name"]}" AS {mv["definition"]};\n')
        output.write("\n")
    
    def _write_data(self, output: TextIO):
        """Write INSERT statements for sampled data."""
        output.write("-- Data\n")
        
        insertion_order = self.resolver.get_insertion_order()
        tables_with_data = [t for t in insertion_order if t in self.results and self.results[t].rows]
        total_tables = len([t for t in insertion_order if t in self.results])
        
        if self.verbose and self.logger:
            self.logger.info(f"    Writing data for {len(tables_with_data)} table(s)...")
        
        for idx, table_name in enumerate(insertion_order, 1):
            if table_name not in self.results:
                continue
            
            result = self.results[table_name]
            table = self.tables[table_name]
            
            if not result.rows:
                if self.verbose and self.logger:
                    percentage = int((idx / total_tables) * 100) if total_tables > 0 else 0
                    progress_bar = self._get_progress_bar(idx, total_tables)
                    self.logger.info(f"      [{idx}/{total_tables}] {progress_bar} {percentage}% - {table_name}: 0 rows (skipped)")
                continue
            
            if self.verbose and self.logger:
                percentage = int((idx / total_tables) * 100) if total_tables > 0 else 0
                progress_bar = self._get_progress_bar(idx, total_tables)
                self.logger.info(f"      [{idx}/{total_tables}] {progress_bar} {percentage}% - {table_name}: {result.row_count} row(s)")
            self._write_table_data(output, table, result)
        
        output.write("\n")
    
    def _get_progress_bar(self, current: int, total: int, width: int = 20) -> str:
        """Generate a simple ASCII progress bar.
        
        Args:
            current: Current progress value
            total: Total value
            width: Width of progress bar in characters
            
        Returns:
            Progress bar string (e.g., "[=====>     ]")
        """
        if total == 0:
            return "[" + " " * width + "]"
        
        filled = int((current / total) * width)
        bar = "=" * filled + ">" * (1 if filled < width else 0) + " " * max(0, width - filled - 1)
        return f"[{bar}]"
    
    def _write_table_data(self, output: TextIO, table: Table, result: SamplingResult):
        """Write INSERT statements for a table."""
        if not result.rows:
            return
        
        # Get column list - include all columns (excluded columns will have NULL)
        # The sampling engine should have already handled column exclusion
        columns = [col["name"] for col in table.columns]
        
        if not columns:
            return
        
        col_list = ", ".join(f'"{col}"' for col in columns)
        
        for row in result.rows:
            values = []
            # Row data should match columns in order (excluded columns are NULL)
            row_data = list(row)
            
            for i, col in enumerate(table.columns):
                if i < len(row_data):
                    value = row_data[i]
                else:
                    value = None
                values.append(self._format_value(value, col["type"]))
            
            value_list = ", ".join(values)
            output.write(f'INSERT INTO "{table.schema}"."{table.name}" ({col_list}) VALUES ({value_list});\n')
    
    def _format_value(self, value: any, pg_type: str) -> str:
        """Format value for SQL output.
        
        Args:
            value: Value to format
            pg_type: PostgreSQL type name
            
        Returns:
            Formatted SQL value string
        """
        if value is None:
            return "NULL"
        
        # Handle JSON/JSONB
        if "json" in pg_type.lower():
            if isinstance(value, (dict, list)):
                return f"'{json.dumps(value).replace(chr(39), chr(39)+chr(39))}'::{pg_type}"
            return f"'{str(value).replace(chr(39), chr(39)+chr(39))}'::{pg_type}"
        
        # Handle strings
        if isinstance(value, str):
            # Escape single quotes
            escaped = value.replace("'", "''").replace("\\", "\\\\")
            return f"'{escaped}'"
        
        # Handle booleans
        if isinstance(value, bool):
            return "TRUE" if value else "FALSE"
        
        # Handle numbers
        if isinstance(value, (int, float)):
            return str(value)
        
        # Handle bytes (bytea)
        if isinstance(value, bytes):
            return f"'\\\\x{value.hex()}'"
        
        # Default: convert to string and escape
        escaped = str(value).replace("'", "''")
        return f"'{escaped}'"
    
    def _write_constraints(self, output: TextIO):
        """Write constraint definitions (PK, UNIQUE, FK)."""
        output.write("-- Constraints\n")
        
        constraint_order = self.resolver.get_constraint_creation_order()
        
        for table_name in constraint_order:
            if table_name not in self.tables:
                continue
            
            table = self.tables[table_name]
            
            # Primary keys
            if table.primary_key:
                pk_cols = ", ".join(f'"{col}"' for col in table.primary_key)
                output.write(f'ALTER TABLE "{table.schema}"."{table.name}" ADD PRIMARY KEY ({pk_cols});\n')
            
            # Unique constraints
            for unique in table.unique_constraints:
                unique_cols = ", ".join(f'"{col}"' for col in unique["columns"])
                output.write(f'ALTER TABLE "{table.schema}"."{table.name}" ADD CONSTRAINT "{unique["name"]}" UNIQUE ({unique_cols});\n')
        
        # Foreign keys (after all tables)
        for table_name in constraint_order:
            if table_name not in self.tables:
                continue
            
            table = self.tables[table_name]
            
            for fk in table.foreign_keys:
                cols = ", ".join(f'"{col}"' for col in fk.columns)
                ref_cols = ", ".join(f'"{col}"' for col in fk.referenced_columns)
                output.write(
                    f'ALTER TABLE "{table.schema}"."{table.name}" '
                    f'ADD CONSTRAINT "{fk.constraint_name}" '
                    f'FOREIGN KEY ({cols}) '
                    f'REFERENCES "{fk.referenced_schema}"."{fk.referenced_table}" ({ref_cols})'
                )
                if fk.on_delete != "NO ACTION":
                    output.write(f" ON DELETE {fk.on_delete}")
                if fk.on_update != "NO ACTION":
                    output.write(f" ON UPDATE {fk.on_update}")
                output.write(";\n")
        
        output.write("\n")
        
        # Sequence setval calls
        self._write_sequence_values(output)
    
    def _write_sequence_values(self, output: TextIO):
        """Write setval calls for sequences based on sampled data.
        
        Handles sequences used by multiple columns by calculating max across all columns.
        """
        
        if self.verbose and self.logger:
            self.logger.info("    Setting sequence values...")
        
        # Find all sequences used by columns in sampled tables
        # Track all columns using each sequence (not just the first one)
        sequence_columns: Dict[str, Dict[str, any]] = {}  # sequence_name -> {schema, name, columns: [(table, column)]}
        
        for table in self.tables.values():
            for col in table.columns:
                # Check for IDENTITY columns (PostgreSQL 10+)
                if col.get("identity") and col.get("identity_sequence"):
                    # IDENTITY column - use the sequence from identity_sequence
                    seq_info = col["identity_sequence"]
                    seq_schema = seq_info["schema"]
                    seq_name = seq_info["name"]
                    qualified_seq = f"{seq_schema}.{seq_name}"
                    
                    if qualified_seq not in sequence_columns:
                        sequence_columns[qualified_seq] = {
                            "schema": seq_schema,
                            "name": seq_name,
                            "columns": [],  # List of (table_name, column_name) tuples
                        }
                    
                    # Add this column to the list (even if sequence already exists)
                    sequence_columns[qualified_seq]["columns"].append(
                        (table.qualified_name, col["name"])
                    )
                
                # Check for SERIAL columns (via nextval() in default)
                default = col.get("default")
                if default:
                    # Extract sequence name from nextval() calls
                    # Pattern: nextval('sequence_name'::regclass) or nextval('schema.sequence_name'::regclass)
                    match = re.search(r"nextval\('([^']+)'::regclass\)", default)
                    if match:
                        seq_name = match.group(1)
                        # Handle schema-qualified sequences
                        if '.' in seq_name:
                            seq_schema, seq_name_only = seq_name.rsplit('.', 1)
                        else:
                            # Default to table's schema
                            seq_schema = table.schema
                            seq_name_only = seq_name
                        
                        qualified_seq = f"{seq_schema}.{seq_name_only}"
                        if qualified_seq not in sequence_columns:
                            sequence_columns[qualified_seq] = {
                                "schema": seq_schema,
                                "name": seq_name_only,
                                "columns": [],  # List of (table_name, column_name) tuples
                            }
                        
                        # Add this column to the list (even if sequence already exists)
                        sequence_columns[qualified_seq]["columns"].append(
                            (table.qualified_name, col["name"])
                        )
        
        if not sequence_columns:
            if self.verbose and self.logger:
                self.logger.info("      No sequences found")
            return
        
        output.write("-- Sequence Values\n")
        
        if self.verbose and self.logger:
            self.logger.info(f"      Processing {len(sequence_columns)} sequence(s)...")
        
        # Calculate max values from sampled data across ALL columns using each sequence
        for seq_qualified, seq_info in sequence_columns.items():
            seq_schema = seq_info["schema"]
            seq_name = seq_info["name"]
            columns_using_seq = seq_info["columns"]
            
            # Find max value across ALL columns using this sequence
            max_value = None
            excluded_columns = []  # Track excluded columns for this sequence
            
            for table_name, column_name in columns_using_seq:
                table = self.tables[table_name]
                
                # Check if this column is excluded
                column_excluded = False
                for pattern in self.exclude_columns:
                    # Check if pattern matches: "table.column", "schema.table.column", or just "column"
                    qualified_col = f"{table_name}.{column_name}"
                    schema_table_col = f"{table.schema}.{table.name}.{column_name}"
                    if (fnmatch.fnmatch(column_name, pattern) or
                        fnmatch.fnmatch(qualified_col, pattern) or
                        fnmatch.fnmatch(schema_table_col, pattern)):
                        column_excluded = True
                        excluded_columns.append((table_name, column_name))
                        break
                
                # If column is excluded, query source database for max value
                if column_excluded:
                    try:
                        with self.conn.cursor() as cur:
                            cur.execute(f"""
                                SELECT MAX("{column_name}")::bigint
                                FROM "{table.schema}"."{table.name}"
                                WHERE "{column_name}" IS NOT NULL
                            """)
                            result = cur.fetchone()
                            if result and result[0] is not None:
                                db_max = int(result[0])
                                if max_value is None or db_max > max_value:
                                    max_value = db_max
                                if self.verbose and self.logger:
                                    self.logger.debug(
                                        f"        Sequence column {table_name}.{column_name} is excluded, "
                                        f"queried max value from source: {db_max}"
                                    )
                    except Exception as e:
                        if self.verbose and self.logger:
                            self.logger.warning(
                                f"        Could not query max value for excluded sequence column "
                                f"{table_name}.{column_name}: {e}"
                            )
                    continue  # Skip checking sampled data for excluded columns
                
                # Check sampled data for non-excluded columns
                if table_name in self.results:
                    result = self.results[table_name]
                    if result.rows:
                        # Find column index
                        col_index = None
                        for i, col in enumerate(table.columns):
                            if col["name"] == column_name:
                                col_index = i
                                break
                        
                        if col_index is not None:
                            # Find max value in this column (handle None values)
                            for row in result.rows:
                                if col_index < len(row) and row[col_index] is not None:
                                    try:
                                        val = int(row[col_index])
                                        if max_value is None or val > max_value:
                                            max_value = val
                                    except (ValueError, TypeError):
                                        # Not a numeric value, skip
                                        pass
            
            # Warn if sequence has excluded columns
            if excluded_columns and self.verbose and self.logger:
                excluded_list = ", ".join(f"{t}.{c}" for t, c in excluded_columns)
                self.logger.info(
                    f"        Sequence {seq_schema}.{seq_name} has excluded columns: {excluded_list}. "
                    f"Max value queried from source database."
                )
            
            # Set sequence value (use max + 1, or 1 if no data)
            # Ensure value is at least 1 (protect against negative values)
            if max_value is not None:
                setval_value = max(1, max_value + 1)
            else:
                # No data sampled, set to 1 (default)
                setval_value = 1
            
            if self.verbose and self.logger:
                col_count = len(seq_info["columns"])
                max_info = f" (max from {max_value} across {col_count} column(s))" if max_value is not None else " (no data, defaulting to 1)"
                self.logger.info(f"        {seq_schema}.{seq_name}: setval({setval_value}){max_info}")
            
            output.write(f"SELECT setval('\"{seq_schema}\".\"{seq_name}\"', {setval_value}, false);\n")
        
        output.write("\n")

